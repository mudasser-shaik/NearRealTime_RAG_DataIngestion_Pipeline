{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "scala_version = '2.12'  # TODO: Ensure this is correct\n",
    "spark_version = '3.5.5'\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:3.2.0',\n",
    "    'org.mongodb.spark:mongo-spark-connector_2.12:10.4.1'\n",
    "]\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"realtime-rag-ingestion-pipeline\") \\\n",
    "    .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, concat, lit, from_json\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, StructType, StructField, IntegerType, LongType\n",
    "from pyspark.sql.functions import from_json, schema_of_json\n",
    "\n",
    "kafka_topic_name = \"product_events\"\n",
    "kafka_bootstrap_servers = \"0.0.0.0:29092\"\n",
    "\n",
    "# Kafka configuration\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": kafka_bootstrap_servers,\n",
    "    \"subscribe\": kafka_topic_name,\n",
    "    \"enable.auto.commit\": \"true\",\n",
    "    \"auto.offset.reset\": \"latest\",\n",
    "    \"max.poll.records\": \"1000\"\n",
    "}\n",
    "\n",
    "# Read from Kafka with JSON deserializer\n",
    "kafkaDf = spark.read.format(\"kafka\") \\\n",
    "    .options(**kafka_options) \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "schema_product = StructType([\n",
    "    StructField(\"store_id\", IntegerType()),\n",
    "    StructField(\"product_id\", IntegerType()),\n",
    "    StructField(\"count\", IntegerType()),\n",
    "    StructField(\"price\", FloatType()),\n",
    "    StructField(\"size\", StringType()),\n",
    "    StructField(\"ageGroup\", StringType()),\n",
    "    StructField(\"gender\", StringType()),\n",
    "    StructField(\"season\", StringType()),\n",
    "    StructField(\"fashionType\", StringType()),\n",
    "    StructField(\"brandName\", StringType()),\n",
    "    StructField(\"baseColor\", StringType()),\n",
    "    StructField(\"articleType\", StringType())\n",
    "])\n",
    "\n",
    "# Deserialize the value column (assuming it contains JSON data)\n",
    "jsonDf = kafkaDf.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "# jsonDf = kafkaDf.selectExpr(\"CAST(value AS STRING)\")\n",
    "jsonDf_new = jsonDf.withColumn(\"value\", from_json(col(\"value\"), schema_product))\n",
    "jsonDf_new.printSchema()\n",
    "jsonDf_new.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9f9767c1911a366"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select individual fields from the JSON\n",
    "queryableDf = jsonDf_new.select(\n",
    "    col(\"key\"),\n",
    "    col(\"value.store_id\"),\n",
    "    col(\"value.product_id\"),\n",
    "    col(\"value.count\"),\n",
    "    col(\"value.price\"),\n",
    "    col(\"value.size\"),\n",
    "    col(\"value.ageGroup\"),\n",
    "    col(\"value.gender\"),\n",
    "    col(\"value.season\"),\n",
    "    col(\"value.fashionType\"),\n",
    "    col(\"value.brandName\"),\n",
    "    col(\"value.baseColor\"),\n",
    "    col(\"value.articleType\")\n",
    ")\n",
    "\n",
    "# Show the schema and data\n",
    "queryableDf.printSchema()\n",
    "queryableDf.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e25b45bdb93052a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "json_data = queryableDf.toJSON().first()\n",
    "print(json_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3d479631635d834"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "jsonDf_transformed = (kafkaDf.selectExpr(\"CAST(value AS STRING)\")\n",
    " .select(from_json(col(\"value\"), schema_product).alias(\"data\")) \\\n",
    "    .select(\"data.*\"))\n",
    "\n",
    "jsonDf_transformed.printSchema()\n",
    "jsonDf_transformed.show(truncate=False)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1eed6acca9f2cc2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import initcap, concat_ws\n",
    "\n",
    "df = jsonDf_transformed.withColumn(\"content\", initcap(concat_ws(' ',\n",
    "                                                col(\"size\"),\n",
    "                                                col(\"ageGroup\"),\n",
    "                                                col(\"gender\"),\n",
    "                                                col(\"season\"),\n",
    "                                                col(\"fashionType\"),\n",
    "                                                col(\"brandName\"),\n",
    "                                                col(\"baseColor\"),\n",
    "                                                col(\"articleType\"),\n",
    "                                                concat_ws('', lit(', price: '), col(\"price\").cast(\"string\")),\n",
    "                                                concat_ws('', lit(', store number: '), col(\"store_id\").cast(\"string\")),\n",
    "                                                concat_ws('', lit(', product id: '), col(\"product_id\").cast(\"string\"))\n",
    "                                                )))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30bc2e1fcad9656a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "734797dd672bb9da"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Embedding Model\n",
    "With RAG, Embeddings are dense vector representations of data (such as text, images, etc.) that capture the semantic meaning of the data. This process allows the system to find and retrieve information based on similarity, even if the search terms aren’t exact matches.\n",
    "\n",
    "Adding extra data, like keywords, tags, or content summaries, can improve these embeddings. Combining this additional context with the main text increases the chances of retrieving relevant information when users ask questions.\n",
    "\n",
    "#For example: \n",
    "If a user searches for “pain relief”, if someone searches for “pain relief,” an embedding that includes related keywords—even if the original text doesn’t mention them—can help them find the correct information. This method ensures that the embeddings reflect a broader context, improving the RAG system's ability to provide accurate and relevant answers.\n",
    "\n",
    "\n",
    "***Using OpenAI API to generate embeddings***\n",
    "Embedding API: https://platform.openai.com/docs/guides/embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e373d8aa1b2cd71"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install --upgrade openai"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9e198b59f2a70a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import openai\n",
    "import os\n",
    "\n",
    "print(openai.__version__)\n",
    "# Set up OpenAI API Key (replace with your actual key)\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\") "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa32fb69b2bb1c54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_embedding(text: str) -> list:\n",
    "    # Generate a 1536-dimensional vector using OpenAI\n",
    "    response = openai.embeddings.create(input=text, model=\"text-embedding-ada-002\")\n",
    "    query_vector = response.data[0].embedding  # Extract the 1536-d vector\n",
    "    return query_vector\n",
    "\n",
    "# Create a UDF from the embedding function\n",
    "get_embeddings_udf = udf(generate_embedding, ArrayType(FloatType()))\n",
    "\n",
    "# Apply the UDF to create df_with_embeddings\n",
    "df_with_embeddings = df.withColumn(\"embedding\", get_embeddings_udf(col(\"content\")))\n",
    "df_with_embeddings.printSchema()\n",
    "df_with_embeddings.show(truncate=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2a9abda17626df3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Write to MongoDB\n",
    "\n",
    "https://www.mongodb.com/docs/spark-connector/current/streaming-mode/streaming-write-config/\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f13f49103c8be2ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the data to vector database - MongoDB atlas\n",
    "# MongoDB configuration\n",
    "mongo_db_user = \"dbuser_genai\"\n",
    "mongo_db_password = \"R6WHZ7MB2KNLCIPC\"\n",
    "mongo_db_host = \"cluster-genai.wcknb.mongodb.net\"\n",
    "mongo_db_options = \"?retryWrites=true&w=majority&appName=Cluster-GenAI\"\n",
    "mongo_db_uri = f\"mongodb+srv://{mongo_db_user}:{mongo_db_password}@{mongo_db_host}/{mongo_db_options}\"\n",
    "mongo_db_name = \"retail\"\n",
    "mongo_collection_name = \"product\"\n",
    "\n",
    "# batch mode\n",
    "# Set up write connection\n",
    "# conf.set(\"spark.mongodb.write.connection.uri\", mongo_db_uri)\n",
    "# conf.set(\"spark.mongodb.write.database\", mongo_db_name)\n",
    "# conf.set(\"spark.mongodb.write.collection\", mongo_collection_name)\n",
    "# # If you need to update instead of inserting :\n",
    "# conf.set(\"spark.mongodb.write.operationType\", \"update\")\n",
    "\n",
    "df_with_embeddings.write \\\n",
    "    .format(\"mongodb\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"spark.mongodb.write.connection.uri\", mongo_db_uri) \\\n",
    "    .option(\"spark.mongodb.write.database\", mongo_db_name) \\\n",
    "    .option(\"spark.mongodb.write.collection\", mongo_collection_name) \\\n",
    "    .save()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d63a16d174b7535"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
